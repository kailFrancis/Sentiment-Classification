{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DOWNLOAD TOTAL DATASET\n",
    "## [BOX SHOULD TAKE APPROX 25 SECONDS TO RUN]\n",
    "\n",
    "# import training data\n",
    "df = pd.read_excel(r'C:\\Users\\kailf\\OneDrive\\Documents\\2023_Summer\\NLP_project\\NLPed_dataset2.xlsx')\n",
    "\n",
    "\n",
    "# function to convert embeddings from string to np.array\n",
    "def string_matrix_to_array(input_string):\n",
    "\n",
    "    # Remove leading and trailing whitespace\n",
    "    input_string = input_string.strip()\n",
    "    \n",
    "    # Remove brackets from the start and end\n",
    "    input_string = input_string.lstrip('[').rstrip(']')\n",
    "    \n",
    "    # Replace line breaks, double spaces and triple spaces with single spaces\n",
    "    input_string = input_string.replace('\\n', ' ')\n",
    "    input_string = input_string.replace('  ', ' ')\n",
    "    input_string = input_string.replace('   ', ' ')\n",
    "\n",
    "    # # replace '] [' with ']['\n",
    "    # input_string = input_string.replace('] [', '][')\n",
    "    \n",
    "    # Split the string into rows based on ']' character\n",
    "    rows = input_string.split(']')\n",
    "    \n",
    "    # Remove any empty strings from the split result\n",
    "    rows = [row.strip() for row in rows if row.strip()]\n",
    "    \n",
    "    # Split each row into values and convert to floats\n",
    "    parsed_rows = []\n",
    "    for row in rows:\n",
    "        row = row.strip('[').strip()\n",
    "        values = [float(val) for val in row.split() if val.strip()]\n",
    "        parsed_rows.append(values)\n",
    "    \n",
    "    # Convert the list of rows into a 2D NumPy array\n",
    "    result_array = np.array(parsed_rows)\n",
    "    \n",
    "    return result_array\n",
    "\n",
    "\n",
    "# change word embeddings from type string to type np.array\n",
    "df['word embeddings'] = df['word embeddings'].apply(string_matrix_to_array)\n",
    "\n",
    "\n",
    "# create test dataframe from df taking one in every 8 rows and remove from df\n",
    "test_df = df.iloc[::8, :]\n",
    "df = df.drop(test_df.index)\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "test_df.reset_index(inplace=True, drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CREATE DATASET AND DATALOADERS\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.data = df['word embeddings'].to_numpy()\n",
    "        self.labels = df['airline_sentiment'].to_numpy()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "# Custom collate function for handling variable-length sequences\n",
    "def custom_collate(batch):\n",
    "    # Get maximum sequence length in the batch\n",
    "    max_seq_len = max([len(seq) for seq, _ in batch])\n",
    "    \n",
    "    # Pad sequences to the maximum length\n",
    "    padded_batch = []\n",
    "    labels = []\n",
    "    \n",
    "    for seq, label in batch:\n",
    "        pad_length = max_seq_len - len(seq)\n",
    "        padded_seq = np.pad(seq, ((0, pad_length), (0, 0)), 'constant')\n",
    "        padded_batch.append(padded_seq)\n",
    "        labels.append(label)\n",
    "    \n",
    "    #### DONT KNOW WHICH ONE TO RETURN\n",
    "    # return np.array(padded_batch), np.array(labels)\n",
    "    return torch.tensor(padded_batch), torch.tensor(labels)\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = MyDataset(df)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.2470204830169678\n",
      "Epoch [2/10], Loss: 0.8078147768974304\n",
      "Epoch [3/10], Loss: 0.7380357384681702\n",
      "Epoch [4/10], Loss: 1.2568457126617432\n",
      "Epoch [5/10], Loss: 0.6585753560066223\n",
      "Epoch [6/10], Loss: 0.660285472869873\n",
      "Epoch [7/10], Loss: 1.257710576057434\n",
      "Epoch [8/10], Loss: 1.5855109691619873\n",
      "Epoch [9/10], Loss: 0.6147798895835876\n",
      "Epoch [10/10], Loss: 1.3316705226898193\n"
     ]
    }
   ],
   "source": [
    "### CREATE MODEL\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_d, hidden_d, layer_d, output_d):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_d\n",
    "        self.layer_dim = layer_d\n",
    "\n",
    "        # LSTM model \n",
    "        self.lstm = nn.LSTM(input_d, hidden_d, layer_d, batch_first=True) \n",
    "\n",
    "        self.fc = nn.Linear(hidden_d, output_d)\n",
    "\n",
    "    def forward(self, x):\n",
    "    \n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        out, _ = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        return out\n",
    "\n",
    "\n",
    "input_dim = 96 # because each word embedding is of length 96\n",
    "output_dim = 3 # because we are classifying into one of 3\n",
    "\n",
    "hidden_dim = 128 # start at 128, explore more later\n",
    "layer_dim = 3 # start with 1, increase more later\n",
    "\n",
    "\n",
    "model = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "\n",
    "error = nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### TRAIN MODEL\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "# change model to float dtype\n",
    "model = model.float()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # iterating through batches\n",
    "    for inputs, labels in train_loader:\n",
    "\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass with float dtype\n",
    "        outputs = model(inputs.float())\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = error(outputs, labels)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Print or log the loss for monitoring training progress\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 54.86%\n",
      "Correct: 553 out of 1008\n",
      "Accuracy: 53.55%\n",
      "Correct: 3775 out of 7049\n"
     ]
    }
   ],
   "source": [
    "# test model\n",
    "test_dataset = MyDataset(test_df)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate)\n",
    "\n",
    "# change model to eval mode\n",
    "model.eval()\n",
    "\n",
    "# keep track of correct predictions\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# iterate through test dataset\n",
    "for inputs, labels in test_loader:\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(inputs.float())\n",
    "    \n",
    "    # Get predictions from the maximum value\n",
    "    _, predicted_labels = torch.max(outputs, 1)\n",
    "    \n",
    "    # Total number of labels in the current batch\n",
    "    total += labels.size(0)\n",
    "    \n",
    "    # Total correct predictions\n",
    "    for label, prediction in zip(labels, predicted_labels):\n",
    "        if label == prediction:\n",
    "            correct += 1\n",
    "\n",
    "# Accuracy\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy: {round(accuracy, 2)}%')\n",
    "print(f'Correct: {correct} out of {total}')\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for inputs, labels in train_loader:\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(inputs.float())\n",
    "    \n",
    "    # Get predictions from the maximum value\n",
    "    _, predicted_labels = torch.max(outputs, 1)\n",
    "    \n",
    "    # Total number of labels in the current batch\n",
    "    total += labels.size(0)\n",
    "    \n",
    "    # Total correct predictions\n",
    "    for label, prediction in zip(labels, predicted_labels):\n",
    "        if label == prediction:\n",
    "            correct += 1\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy: {round(accuracy, 2)}%')\n",
    "print(f'Correct: {correct} out of {total}')\n",
    "\n",
    "# 1 epoch gives 54.86"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
